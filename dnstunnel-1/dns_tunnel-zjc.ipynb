{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/admin/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import json\n",
    "#j = json.loads('{\"one\" : \"1\", \"two\" : \"2\", \"three\" : \"3\"}')\n",
    "import time\n",
    "\n",
    "from tldextract import TLDExtract\n",
    "extract = TLDExtract(suffix_list_urls=None)\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Bidirectional, Conv1D, MaxPool1D, Flatten\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_IP_IDX = 3-1               \n",
    "DST_IP_IDX = 4-1               \n",
    "SRC_PORT_IDX = 5-1             \n",
    "DST_PORT_IDX = 6-1             \n",
    "PROTOCOL_IDX = 7-1             \n",
    "DNS_QUERY_NAME_IDX = 55-1 # domain\n",
    "DNS_REQUEST_TYPE = 56-1\n",
    "DNS_DOMAIN_TTL = 59-1\n",
    "DNS_REPLY_IPV4IP = 60-1        \n",
    "DNS_REPLY_IPV6IP = 61-1        \n",
    "DNS_REPLY_RRTYPE = 62-1        \n",
    "DNS_REQUEST_LEN  = 88-1        \n",
    "DNS_REPLY_LENGTH = 90-1\n",
    "\n",
    "def iterbrowse(path):\n",
    "    for home, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            yield os.path.join(home, filename)\n",
    "            \n",
    "def extract_domain(domain):\n",
    "    suffix = {'.com','.la','.io', '.co', '.cn','.info', '.net',\n",
    "              '.org','.me', '.mobi', '.us', '.biz', '.xxx', '.ca',\n",
    "              '.co.jp', '.com.cn', '.net.cn', '.org.cn', '.mx','.tv',\n",
    "              '.ws', '.ag', '.com.ag', '.net.ag', '.org.ag','.am',\n",
    "              '.asia', '.at', '.be', '.com.br', '.net.br', '.name', \n",
    "              '.live', '.news', '.bz', '.tech', '.pub', '.wang', \n",
    "              '.space', '.top', '.xin', '.social', '.date', '.site', \n",
    "              '.red', '.studio', '.link', '.online', '.help', '.kr', \n",
    "              '.club', '.com.bz', '.net.bz', '.cc', '.band', '.market',\n",
    "              '.com.co', '.net.co', '.nom.co', '.lawyer', '.de', '.es',\n",
    "              '.com.es', '.nom.es', '.org.es', '.eu', '.wiki', \n",
    "              '.design', '.software', '.fm', '.fr', '.gs', '.in', \n",
    "              '.co.in', '.firm.in', '.gen.in', '.ind.in', '.net.in', \n",
    "              '.org.in', '.it', '.jobs', '.jp', '.ms', '.com.mx', '.nl',\n",
    "              '.nu','.co.nz','.net.nz', '.org.nz', '.se', '.tc', '.tk',\n",
    "              '.tw', '.com.tw', '.idv.tw', '.org.tw', '.hk', '.co.uk',\n",
    "              '.me.uk', '.org.uk', '.vg','.in-addr.arpa'}\n",
    "\n",
    "    domain = domain.lower()\n",
    "    names = domain.split(\".\")\n",
    "    if len(names) >= 3:\n",
    "        if (\".\"+\".\".join(names[-2:])) in suffix:\n",
    "            return \".\".join(names[-3:]), \".\".join(names[:-3])\n",
    "        elif (\".\"+names[-1]) in suffix:\n",
    "            return \".\".join(names[-2:]), \".\".join(names[:-2])\n",
    "    #print (\"New domain suffix found. Use tld extract domain...\")\n",
    "\n",
    "    pos = domain.rfind(\"/\")\n",
    "    if pos >= 0: # maybe subdomain contains /, for dns tunnel tool\n",
    "        ext = extract(domain[pos+1:])\n",
    "        subdomain = domain[:pos+1] + ext.subdomain\n",
    "    else:\n",
    "        ext = extract(domain)\n",
    "        subdomain = ext.subdomain\n",
    "    if ext.suffix:\n",
    "        mdomain = ext.domain + \".\" + ext.suffix\n",
    "    else:\n",
    "        mdomain = ext.domain\n",
    "    return mdomain, subdomain\n",
    "\n",
    "\n",
    "def filter_metadata_dns(data):\n",
    "    if(len(data) < 91):\n",
    "        return False\n",
    "\n",
    "    protol  = data[PROTOCOL_IDX]\n",
    "    dstport = data[DST_PORT_IDX]\n",
    "    dstip   = data[DST_IP_IDX]\n",
    "    qname   = data[DNS_QUERY_NAME_IDX]\n",
    "\n",
    "    if '' == qname or '' == dstip:\n",
    "        return False\n",
    "    if '17' == protol and ('53' == dstport):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def metadata2_domain_data(log): \n",
    "    data = log.split('^')\n",
    "    if not filter_metadata_dns(data):\n",
    "        return None, None\n",
    "    domain = data[DNS_QUERY_NAME_IDX]\n",
    "    mdomain, subdomain = extract_domain(domain)\n",
    "    return (mdomain, subdomain)\n",
    "\n",
    "\n",
    "def get_local_data(tag=\"labeled\"):\n",
    "    data_path = \"./sample_data\"\n",
    "    black_data, white_data = [], []    \n",
    "    for dir_name in (\"black\", \"cdn\", \"white\"):\n",
    "        dir_path = \"%s/%s_%s\" % (data_path, tag, dir_name)\n",
    "\n",
    "        for path in iterbrowse(dir_path):\n",
    "            print( path)\n",
    "            with open(path) as f:\n",
    "                for line in f:\n",
    "                    mdomain, subdomain = metadata2_domain_data(line)\n",
    "                    if subdomain is not None:\n",
    "                        if \"white\" in path:\n",
    "                            white_data.append(subdomain)\n",
    "                        elif \"cdn\" in path:\n",
    "                            white_data.append(subdomain)\n",
    "                        elif \"black\" in path and \"pcap\" in path:\n",
    "                            black_data.append(subdomain)\n",
    "                        else:\n",
    "                            pass\n",
    "                            #print (\"pass path:\", path)\n",
    "                    #else:\n",
    "                    #    print (\"unknown line:\", line, \" in file:\", path)\n",
    "    return black_data, white_data\n",
    "\n",
    "\n",
    "class LABEL(object):\n",
    "    white = 0\n",
    "    cdn = 0\n",
    "    black = 1\n",
    "\n",
    "def pad_sequences(X, maxlen, value=0):\n",
    "    S=[]\n",
    "    for x in X:\n",
    "        xlen = len(x)\n",
    "        if xlen < maxlen:\n",
    "            x.extend([value]*(maxlen-xlen))\n",
    "        else:\n",
    "            x = x[:maxlen]\n",
    "        S.append(x)\n",
    "    return S\n",
    "\n",
    "def get_data():\n",
    "    black_x, white_x = get_local_data()\n",
    "    black_y, white_y = [LABEL.black]*len(black_x),[LABEL.white]*len(white_x)\n",
    "\n",
    "    X = black_x + white_x\n",
    "    labels = black_y + white_y\n",
    "    print(X[0])\n",
    "    # Generate a dictionary of valid characters\n",
    "    valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}\n",
    "\n",
    "    max_features = len(valid_chars) + 1\n",
    "    print(\"max_features:\", max_features)\n",
    "    maxlen = np.max([len(x) for x in X])\n",
    "    print(\"max_len:\", maxlen)\n",
    "    maxlen = min(maxlen, 256)\n",
    "\n",
    "    # Convert characters to int and pad\n",
    "    X = [[valid_chars[y] for y in x] for x in X]\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0.)\n",
    "\n",
    "    # Convert labels to 0-1\n",
    "    Y = labels\n",
    "    \n",
    "    volcab_file = \"volcab.pkl\"\n",
    "    output = open(volcab_file, 'wb') \n",
    "    # Pickle dictionary using protocol 0.\n",
    "    data = {\"valid_chars\": valid_chars,\n",
    "            \"max_len\": maxlen, \n",
    "            \"volcab_size\": max_features}\n",
    "    pickle.dump(data, output)\n",
    "    output.close()\n",
    "\n",
    "    return X, Y, maxlen, max_features\n",
    "\n",
    "def build_model_BiRNN(max_len, volcab_size):\n",
    "    \"\"\"Build Bi-RNN model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=volcab_size,\n",
    "                        output_dim=64,\n",
    "                        input_length=max_len))\n",
    "    #model.add(Bidirectional(GRU(16)))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_cnn_model(max_len, volcab_size):\n",
    "    \"\"\"Build CNN model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=volcab_size,\n",
    "                        output_dim=64,\n",
    "                        input_length=max_len))\n",
    "    model.add(Conv1D(128,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation=\"relu\", \n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     activity_regularizer=regularizers.l1(0.01)))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Conv1D(128,\n",
    "                     4,\n",
    "                     padding='valid',\n",
    "                     activation=\"relu\", \n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     activity_regularizer=regularizers.l1(0.01)))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Conv1D(128,\n",
    "                     5,\n",
    "                     padding='valid',\n",
    "                     activation=\"relu\", \n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     activity_regularizer=regularizers.l1(0.01)))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16,activation=\"relu\"))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def run():\n",
    "    X, Y, max_len, volcab_size = get_data()\n",
    "    \n",
    "    print( \"X len:\", len(X), \"Y len:\", len(Y))\n",
    "    trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "    print( trainX[:1])\n",
    "    print( trainY[:1])\n",
    "    print( testX[-1:])\n",
    "    print( testY[-1:])\n",
    "\n",
    "    model = get_cnn_model(max_len, volcab_size)\n",
    "    model.fit(trainX, trainY, \n",
    "              validation_data=(testX, testY), \n",
    "              verbose=1,\n",
    "              batch_size=32)\n",
    "\n",
    "    \n",
    "    filename = 'finalized_model.keras'\n",
    "    model.save(filename)\n",
    "\n",
    "    model.load(filename)\n",
    "    print( \"Just review 3 sample data test result:\")\n",
    "    result = model.predict(testX[0:3])\n",
    "    print( result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sample_data/labeled_black/dns2tcp_cmd.pcap.txt\n",
      "./sample_data/labeled_black/dns2tcp_sendfile9.pcap.txt\n",
      "./sample_data/labeled_black/dns2tcp_when_use_ssh.txt\n",
      "./sample_data/labeled_black/dnscapy_scp.pcap.txt\n",
      "./sample_data/labeled_black/dnscapy_scp2.pcap.txt\n",
      "./sample_data/labeled_black/dnscat2_when_exec_command_rm_file.txt\n",
      "./sample_data/labeled_black/dnscat2_when_idle.txt\n",
      "./sample_data/labeled_black/download_dnscat2_file12.pcap.txt\n",
      "./sample_data/labeled_black/download_dnscat2_file13.pcap.txt\n",
      "./sample_data/labeled_black/download_dnscat2_file14.pcap.txt\n",
      "./sample_data/labeled_black/iodine_direct_scp17_base128.pcap.txt\n",
      "./sample_data/labeled_black/iodine_direct_scp3_base64.pcap.txt\n",
      "./sample_data/labeled_black/iodine_direct_ssh4_base32.pcap.txt\n",
      "./sample_data/labeled_black/iodine_direct_ssh6_base128.pcap.txt\n",
      "./sample_data/labeled_black/iodine_direct_ssh6_base64.pcap.txt\n",
      "./sample_data/labeled_black/iodine_direct_ssh6_base64u.pcap.txt\n",
      "./sample_data/labeled_black/iodine_direct_ssh9_base32_again.pcap.txt\n",
      "./sample_data/labeled_black/iodine_idle_direct_idle44.pcap.txt\n",
      "./sample_data/labeled_black/nbtoo_dnscat_file7.pcap.txt\n",
      "./sample_data/labeled_black/ozyman_idle.pcap.txt\n",
      "./sample_data/labeled_black/ozyman_idle2.pcap.txt\n",
      "./sample_data/labeled_black/ozyman_idle3.pcap.txt\n",
      "./sample_data/labeled_black/tcp-over-dns-idle.pcap.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-ctripgslb.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-dlgslb.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-gosuncdn.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-mccdnglb.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-mmycdn.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-ruisucdn.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-spcdntip.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-0-tcdnvod.com.txt\n",
      "./sample_data/labeled_cdn/2017-8-2-8-tcdnvod.com.txt\n",
      "./sample_data/labeled_white/2017-8-15-0-henanpeace.org.cn.txt\n",
      "./sample_data/labeled_white/2017-8-15-8-qichedaquan.com.txt\n",
      "./sample_data/labeled_white/2017-8-16-11-lse.ac.uk.txt\n",
      "./sample_data/labeled_white/2017-8-16-9-dicp.ac.cn.txt\n",
      "./sample_data/labeled_white/2017-8-2-0-365yg.com.txt\n",
      "./sample_data/labeled_white/2017-8-2-0-bilibiligame.net.txt\n",
      "{'n': 1, 'p': 2, 'f': 3, 'z': 4, 'l': 5, 'm': 6, 'u': 7, 'v': 8, '3': 9, '6': 10, '=': 11, 'r': 12, '+': 13, '\\\\': 14, 'k': 15, 't': 16, '.': 17, '1': 18, 'y': 19, 'x': 20, '7': 21, 'j': 22, '4': 23, '8': 24, '2': 25, 'd': 26, 'q': 27, 'a': 28, '9': 29, 'g': 30, 'e': 31, 'c': 32, 's': 33, 'b': 34, '/': 35, '-': 36, '5': 37, 'h': 38, 'o': 39, 'w': 40, 'i': 41, '0': 42}\n",
      "max_features: 43\n",
      "max_len: 942\n",
      "aaaaacdmqa.=auth.a\n",
      "[28, 28, 28, 28, 28, 32, 26, 6, 27, 28, 17, 11, 28, 7, 16, 38, 17, 28]\n"
     ]
    }
   ],
   "source": [
    "black_x, white_x = get_local_data()\n",
    "black_y, white_y = [LABEL.black]*len(black_x),[LABEL.white]*len(white_x)\n",
    "\n",
    "X = black_x + white_x\n",
    "labels = black_y + white_y\n",
    "# Generate a dictionary of valid characters\n",
    "valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}\n",
    "print(valid_chars)\n",
    "max_features = len(valid_chars) + 1\n",
    "print(\"max_features:\", max_features)\n",
    "maxlen = np.max([len(x) for x in X])\n",
    "print(\"max_len:\", maxlen)\n",
    "maxlen = min(maxlen, 256)\n",
    "print(X[0])\n",
    "X = [[valid_chars[y] for y in x] for x in X]\n",
    "print(X[0])\n",
    "X = pad_sequences(X, maxlen=maxlen, value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X len: 4439 Y len: 4439\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print( \"X len:\", len(X), \"Y len:\", len(Y))\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "print(type(trainX))\n",
    "#trainX, testX, trainY, testY = train_test_split(trainX, trainY, test_size=0.2, \n",
    "                                                    #random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainX))\n",
    "print(type(trainY))\n",
    "trainX=np.mat(trainX)\n",
    "testX=np.mat(testX)\n",
    "\n",
    "trainY=np.mat(trainY).flatten().T\n",
    "testY=np.mat(testY).flatten().T\n",
    "print(type(trainX))\n",
    "print(type(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3551\n",
      "3551\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 256, 64)           2752      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 103,649\n",
      "Trainable params: 103,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = get_cnn_model(max_len, volcab_size)\n",
    "\n",
    "model = build_model_BiRNN(max_len, volcab_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3551 samples, validate on 888 samples\n",
      "Epoch 1/20\n",
      "3551/3551 [==============================] - 17s 5ms/step - loss: 0.6924 - acc: 0.5463 - val_loss: 0.6828 - val_acc: 0.5507\n",
      "Epoch 2/20\n",
      "3551/3551 [==============================] - 14s 4ms/step - loss: 0.6781 - acc: 0.5928 - val_loss: 0.6771 - val_acc: 0.6216\n",
      "Epoch 3/20\n",
      "3551/3551 [==============================] - 14s 4ms/step - loss: 0.6671 - acc: 0.6345 - val_loss: 0.6771 - val_acc: 0.6284\n",
      "Epoch 4/20\n",
      "3551/3551 [==============================] - 14s 4ms/step - loss: 0.6649 - acc: 0.6539 - val_loss: 0.6714 - val_acc: 0.6284\n",
      "Epoch 5/20\n",
      "3551/3551 [==============================] - 14s 4ms/step - loss: 0.6664 - acc: 0.6536 - val_loss: 0.6647 - val_acc: 0.6284\n",
      "Epoch 6/20\n",
      "3551/3551 [==============================] - 14s 4ms/step - loss: 0.6554 - acc: 0.6539 - val_loss: 0.6479 - val_acc: 0.6306\n",
      "Epoch 7/20\n",
      "3551/3551 [==============================] - 13s 4ms/step - loss: 0.6395 - acc: 0.6542 - val_loss: 1.0133 - val_acc: 0.6881\n",
      "Epoch 8/20\n",
      "3551/3551 [==============================] - 14s 4ms/step - loss: 1.0213 - acc: 0.7147 - val_loss: 0.6523 - val_acc: 0.6306\n",
      "Epoch 9/20\n",
      "3551/3551 [==============================] - 15s 4ms/step - loss: 0.6445 - acc: 0.6542 - val_loss: 0.6451 - val_acc: 0.6306\n",
      "Epoch 10/20\n",
      "3551/3551 [==============================] - 17s 5ms/step - loss: 0.6367 - acc: 0.6542 - val_loss: 0.6399 - val_acc: 0.6306\n",
      "Epoch 11/20\n",
      "3551/3551 [==============================] - 16s 4ms/step - loss: 0.6339 - acc: 0.6542 - val_loss: 0.6341 - val_acc: 0.6306\n",
      "Epoch 12/20\n",
      "3551/3551 [==============================] - 13s 4ms/step - loss: 0.6294 - acc: 0.6539 - val_loss: 0.6275 - val_acc: 0.6318\n",
      "Epoch 13/20\n",
      "3551/3551 [==============================] - 13s 4ms/step - loss: 0.6212 - acc: 0.6542 - val_loss: 0.6199 - val_acc: 0.6318\n",
      "Epoch 14/20\n",
      "3551/3551 [==============================] - 13s 4ms/step - loss: 0.6136 - acc: 0.6564 - val_loss: 0.6044 - val_acc: 0.6340\n",
      "Epoch 15/20\n",
      "3551/3551 [==============================] - 13s 4ms/step - loss: 0.5942 - acc: 0.6778 - val_loss: 0.4845 - val_acc: 0.7827\n",
      "Epoch 16/20\n",
      "3551/3551 [==============================] - 13s 4ms/step - loss: 0.5684 - acc: 0.7564 - val_loss: 0.6385 - val_acc: 0.6306\n",
      "Epoch 17/20\n",
      "3551/3551 [==============================] - 16s 5ms/step - loss: 0.6329 - acc: 0.6542 - val_loss: 0.6337 - val_acc: 0.6306\n",
      "Epoch 18/20\n",
      "3551/3551 [==============================] - 17s 5ms/step - loss: 0.6287 - acc: 0.6539 - val_loss: 0.6304 - val_acc: 0.6306\n",
      "Epoch 19/20\n",
      "3551/3551 [==============================] - 15s 4ms/step - loss: 0.6253 - acc: 0.6542 - val_loss: 0.6275 - val_acc: 0.6306\n",
      "Epoch 20/20\n",
      "3551/3551 [==============================] - 13s 4ms/step - loss: 0.6211 - acc: 0.6539 - val_loss: 0.6251 - val_acc: 0.6306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2d8f804a8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, \n",
    "          validation_data=(testX, testY), \n",
    "          verbose=1,\n",
    "          batch_size=3551,\n",
    "          epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime(time.time()))\n",
    "\n",
    "model.save(\"./models/BiLST-\"+timestamp+\".module\")\n",
    "model.save(\"./models/BiLST-final.module\")\n",
    "\n",
    "print(\"Just review 2 sample data test result:\")\n",
    "\n",
    "result = model.predict_classes(testX[0:2])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def get_predict_data():\n",
    "    data_path = \"./xshell_data\"\n",
    "    black_data = []\n",
    "    for path in iterbrowse(data_path):\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                mdomain, subdomain = metadata2_domain_data(line)\n",
    "                if subdomain is not None:\n",
    "                    black_data.append(subdomain)\n",
    "    return black_data\n",
    "\n",
    "\n",
    "org_X = []\n",
    "\n",
    "def get_xshell_data():\n",
    "    global org_X\n",
    "    org_X = get_predict_data()\n",
    "    labels = [LABEL.black]*len(org_X)\n",
    "\n",
    "    volcab_file = \"volcab.pkl\"\n",
    "    assert os.path.exists(volcab_file)\n",
    "    pkl_file = open(volcab_file, 'rb')\n",
    "    data = pickle.load(pkl_file)\n",
    "    valid_chars, maxlen, max_features = data[\"valid_chars\"], data[\"max_len\"], data[\"volcab_size\"]\n",
    "\n",
    "    # Convert characters to int and pad\n",
    "    X = [[valid_chars[y] if y in valid_chars else 0 for y in x] for x in org_X]\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0.)\n",
    "\n",
    "    # Convert labels to 0-1\n",
    "    Y = labels\n",
    "    return X, Y, maxlen, max_features\n",
    "\n",
    "\n",
    "def run():\n",
    "    testX, testY, max_len, volcab_size = get_xshell_data()\n",
    "    print( \"X len:\", len(testX), \"Y len:\", len(testY))\n",
    "    print( testX[-1:])\n",
    "    print( testY[-1:])\n",
    "\n",
    "    filename = 'finalized_model.tflearn'\n",
    "    model = load_model(\"./models/BiLST-final.module\")\n",
    "\n",
    "    predictions = model.predict(testX)\n",
    "    \n",
    "    cnt = 0\n",
    "    global org_X\n",
    "    for i,p in enumerate(predictions):\n",
    "        #if abs(p[2]-testY[i][2]) < 0.1:\n",
    "        if p[2]>p[1] and p[1]>p[0]:\n",
    "            cnt += 1\n",
    "        else:\n",
    "            print( \"found data not detected:\")\n",
    "            print( \"original subdomain:\", org_X[i])\n",
    "            print( \"prediction compare:\", p, testY[i])\n",
    "    print( \"Dectected cnt:\", cnt, \"total:\", len(predictions))\n",
    "    print( \"Dectect Rate is:\", cnt/(len(predictions)+.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY, max_len, volcab_size = get_xshell_data()\n",
    "print( \"X len:\", len(testX), \"Y len:\", len(testY))\n",
    "print( testX[-1:])\n",
    "print( testY[-1:])\n",
    "testX = np.mat(testX)\n",
    "testY = np.mat(testY)\n",
    "print(type(testX))\n",
    "print(type(testY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./models/BiLST-final.module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "for i,p in enumerate(predictions):\n",
    "    #print(i,p)\n",
    "    #if abs(p[2]-testY[i][2]) < 0.1:\n",
    "    if p[0]==1:\n",
    "        cnt += 1\n",
    "    else:\n",
    "        continue\n",
    "        print( \"found data not detected:\")\n",
    "        print( \"original subdomain:\", org_X[i])\n",
    "        #print( \"prediction compare:\", p[0], testY[i])\n",
    "print( \"Dectected cnt:\", cnt, \"total:\", len(predictions))\n",
    "print( \"Dectect Rate is:\", cnt/(len(predictions)+.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "\n",
    "#model = load_model(\"./models/BiLST-20180928-183836.module\")\n",
    "\n",
    "volcab_file = \"volcab.pkl\"\n",
    "pkl_file = open(volcab_file, 'rb')\n",
    "data = pickle.load(pkl_file)\n",
    "valid_chars, maxlen, max_features = data[\"valid_chars\"], data[\"max_len\"], data[\"volcab_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-24ee25796040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconsumer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaConsumer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# message value and key are raw bytes -- decode if necessary!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# e.g., for unicode: `message.value.decode('utf-8')`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdns_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *topics, **configs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                         str(self.config['api_version']), str_version)\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# Get auto-discovered version from client if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mcheck_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version_auto_timeout_ms'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mcheck_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0mtry_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_id\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleast_loaded_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtry_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoBrokersAvailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtry_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtry_node\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "consumer = KafkaConsumer('dns')\n",
    "for message in consumer:\n",
    "    # message value and key are raw bytes -- decode if necessary!\n",
    "    # e.g., for unicode: `message.value.decode('utf-8')`\n",
    "    dns_log = json.loads(message.value.decode('utf-8'))\n",
    "    dns_query = dns_log[\"query\"]\n",
    "    \n",
    "    org_X = extract_domain(dns_query)\n",
    "    X = [[valid_chars[y] if y in valid_chars else 0 for y in x] for x in org_X]\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0.)\n",
    "    \n",
    "    X = np.mat(X)\n",
    "    \n",
    "    rs = model.predict_classes(X)[0][0]\n",
    "    \n",
    "    print(rs, dns_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [12,13,4,5]\n",
    "b = [1,22,345,4]\n",
    "c = a + b\n",
    "print(c)\n",
    "a.pop()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pad_sequences([1,4,5],10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
